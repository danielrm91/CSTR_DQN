{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783b6ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drangelm\\.conda\\envs\\notebook\\Lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "class CSTRCoolingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CSTRCoolingEnv, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.V = 100             # Reactor volume (L)\n",
    "        self.q = 20             # Flowrate (L/min)\n",
    "        self.Caf = 1.0           # Feed concentration (mol/L)\n",
    "        self.Tf = 350            # Feed temperature (K)\n",
    "        self.k0 = 15.2e10         # 1/min\n",
    "        self.EoverR = 10000       # K\n",
    "        self.mdelH = 5e4         # J/mol\n",
    "        self.rho = 1000          # g/L\n",
    "        self.Cp = 0.239          # J/g-K\n",
    "        self.UA = 5e4            # J/min-K\n",
    "        self.Tj_base = 300       # K\n",
    "        self.Tj_delta = 30        # delta per action level\n",
    "        self.dt = 1.0            # time per step (min)\n",
    "        self.max_steps = 200\n",
    "        self.possible_setpoints = [290,300,310,320,330,340,350,360,370]\n",
    "\n",
    "        # Normalization ranges\n",
    "        self.Ca_min = 0.0\n",
    "        self.Ca_max = 1.2      # expected max around feed conc.\n",
    "        \n",
    "        self.T_min = 300.0\n",
    "        self.T_max = 400.0      # may vary depending on dynamics\n",
    "\n",
    "\n",
    "        # Action space: 3 discrete cooling settings\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        # Observation space: [Ca, T]\n",
    "        low_obs = np.array([0.0, 300.0, 300.0], dtype=np.float32)\n",
    "        high_obs = np.array([2.0, 500.0, 500.0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low_obs, high=high_obs, dtype=np.float32)\n",
    "\n",
    "        self.state = None\n",
    "        self.steps = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        Ca, T, sp = self.state\n",
    "\n",
    "        sp = self.setpoints[self.steps]\n",
    "\n",
    "        # Determine Tj based on action\n",
    "        if action == 0:\n",
    "            Tj = self.Tj_base + 2*self.Tj_delta\n",
    "        elif action == 1:\n",
    "            Tj = self.Tj_base + self.Tj_delta\n",
    "        elif action == 2:\n",
    "            Tj = self.Tj_base\n",
    "        elif action == 3:\n",
    "            Tj = self.Tj_base - self.Tj_delta\n",
    "        elif action == 4:\n",
    "            Tj = self.Tj_base - 2*self.Tj_delta\n",
    "\n",
    "        # Integrate ODEs for 1 time step\n",
    "        def cstr_odes(t, y):\n",
    "            Ca, T = y\n",
    "            rA = self.k0 * np.exp(-self.EoverR / T) * Ca\n",
    "            dCadt = self.q / self.V * (self.Caf - Ca) - rA\n",
    "            dTdt = (self.q / self.V * (self.Tf - T)\n",
    "                    + (-self.mdelH) / (self.rho * self.Cp) * rA\n",
    "                    + self.UA / (self.rho * self.Cp * self.V) * (Tj - T))\n",
    "            return [dCadt, dTdt]\n",
    "\n",
    "        sol = solve_ivp(cstr_odes, [0, self.dt], [Ca, T], method='RK45')\n",
    "        Ca_next, T_next = sol.y[:, -1]\n",
    "\n",
    "        self.state = np.array([Ca_next, T_next, sp], dtype=np.float32)\n",
    "\n",
    "        # Reward: penalize deviation from setpoint\n",
    "        reward = (-abs(T_next - sp))/100\n",
    "\n",
    "        self.steps += 1\n",
    "        done = self.steps >= self.max_steps\n",
    "\n",
    "        obs = self.normalize_state(*self.state)\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def generate_sectioned_list(self, C, num_sections=10, section_size=20):\n",
    "        result = []\n",
    "        for _ in range(num_sections):\n",
    "            value = random.choice(C)\n",
    "            section = [value] * section_size\n",
    "            result.extend(section)\n",
    "        return result\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        Ca0 = 1.0 \n",
    "        T0 = 350.0 + self.np_random.uniform(-5.0, 5.0)  #5\n",
    "        sp0 = 300.0\n",
    "        C = self.possible_setpoints\n",
    "        self.setpoints = self.generate_sectioned_list(C)\n",
    "        self.state = np.array([Ca0, T0, sp0], dtype=np.float32)\n",
    "        obs = self.normalize_state(*self.state)\n",
    "        return obs, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        Ca, T = self.state\n",
    "        print(f\"Step: {self.steps}, Ca: {Ca:.3f}, T: {T:.2f}\")\n",
    "\n",
    "    def normalize_state(self, Ca, T, sp):\n",
    "        Ca_norm = (Ca - self.Ca_min) / (self.Ca_max - self.Ca_min)\n",
    "        T_norm = (T - self.T_min) / (self.T_max - self.T_min)\n",
    "        sp_norm = (sp - self.T_min) / (self.T_max - self.T_min)\n",
    "        return np.array([Ca_norm, T_norm, sp_norm], dtype=np.float32)\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3afc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drangelm\\.conda\\envs\\notebook\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on CUDA\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reward(-40.4432), Epsilon(0.97051):   1%|          | 15/2000 [00:06<14:53,  2.22it/s] "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Deep Q Learning\n",
    "# Daniel Rangel-Martinez\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size =           100\n",
    "learning_rate =         5e-5\n",
    "gamma =                 0.99\n",
    "update_target_freq =    10\n",
    "minibatch_size =        64\n",
    "buff_size =             20_000\n",
    "train_after =           50\n",
    "episodes =              2000\n",
    "train_epochs =          5\n",
    "test_eps =              7\n",
    "target_update_freq =    3\n",
    "start_epsilon =         1.0\n",
    "end_epsilon =           0.01\n",
    "\n",
    "# Torch helper class\n",
    "class TorchHelper:\n",
    "    def __init__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Working on CUDA\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Working on CUDA\")\n",
    "    \n",
    "    def f(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "    def i(self, x):\n",
    "        return torch.tensor(x, dtype=torch.int32).to(self.device)\n",
    "    \n",
    "    def l(self, x):\n",
    "        return torch.tensor(x, dtype=torch.long).to(self.device)\n",
    "    \n",
    "t = TorchHelper()\n",
    "device = t.device\n",
    "\n",
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, N):\n",
    "        self.buf = collections.deque(maxlen=N)\n",
    "\n",
    "    def add(self, s, a, r, s2, d):\n",
    "        self.buf.append((s, a, r, s2, d))\n",
    "\n",
    "    def sample(self, n):\n",
    "        minibatch = random.sample(self.buf, n)\n",
    "        S, A, R, S2, D = zip(*minibatch)\n",
    "        return t.f(S), t.l(A), t.f(R), t.f(S2), t.i(D)\n",
    "\n",
    "# Create all the modules needed \n",
    "def create_modules():\n",
    "    # Environment\n",
    "    env_1 = CSTRCoolingEnv()\n",
    "    env_2 = CSTRCoolingEnv()\n",
    "    \n",
    "    # Replay Buffer\n",
    "    buffer = ReplayBuffer(buff_size)\n",
    "\n",
    "    # Q-Network\n",
    "    Q = torch.nn.Sequential(\n",
    "        torch.nn.Linear(obs_size, hidden_size),\n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.Linear(hidden_size, act_size)\n",
    "    ).to(device)\n",
    "\n",
    "    # Target Q-Network\n",
    "    Q_t = torch.nn.Sequential(\n",
    "        torch.nn.Linear(obs_size, hidden_size),\n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.Linear(hidden_size, act_size)\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(Q.parameters(), lr = learning_rate)\n",
    "    \n",
    "    return env_1, env_2, buffer, Q, Q_t, optimizer\n",
    "\n",
    "# Target network update\n",
    "def update_target(target, central):\n",
    "    for t_params, c_params in zip(target.parameters(), central.parameters()):\n",
    "        t_params.data.copy_(c_params)\n",
    "\n",
    "# Epsilon-Greedy Decision Making Process\n",
    "def policy(state, Q, epsilon):\n",
    "    state = t.f(state).view(-1, obs_size)\n",
    "    if np.random.rand() < epsilon:\n",
    "        act = np.random.randint(act_size)\n",
    "        return act\n",
    "    else:\n",
    "        act = torch.argmax(Q(state)).item()\n",
    "        return act\n",
    "\n",
    "# Training update\n",
    "def update_Qnetworks(episode, buffer, Q, Qt, optimizer):\n",
    "    s,a,r,s_,done = buffer.sample(minibatch_size)\n",
    "    # Prediction with model\n",
    "    q_values = Q(s).gather(1, a.view(-1,1)).squeeze()\n",
    "    # Sampling and calculation of Q\n",
    "    qt_values = Qt(s_).max(dim=1).values\n",
    "    targets = r + gamma * qt_values * (1-done)\n",
    "    # Calculate the error\n",
    "    loss = torch.nn.MSELoss()(targets.detach(),q_values)\n",
    "\n",
    "    #Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step\n",
    "\n",
    "    # Update the target Q-Network\n",
    "    if episode % update_target_freq == 0:\n",
    "        update_target(Qt, Q)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def play_episode(env, Q, epsilon, buf=None, max_steps=100):\n",
    "    obs, _ = env.reset(seed=123)\n",
    "    done = False\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        act = policy(obs, Q, epsilon)\n",
    "        next_obs, reward, done, _ = env.step(act)\n",
    "\n",
    "        if buf is not None:\n",
    "            buf.add(obs, act, reward, next_obs, done)\n",
    "\n",
    "        states.append(obs)\n",
    "        actions.append(act)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards\n",
    "\n",
    "# Training Function\n",
    "def train():\n",
    "    global obs_size, act_size\n",
    "\n",
    "    env = CSTRCoolingEnv()\n",
    "    test_env = CSTRCoolingEnv()\n",
    "\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    act_size = env.action_space.n\n",
    "\n",
    "    Q = torch.nn.Sequential(\n",
    "        torch.nn.Linear(obs_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_size, act_size)\n",
    "    ).to(device)\n",
    "\n",
    "    Qt = torch.nn.Sequential(\n",
    "        torch.nn.Linear(obs_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_size, act_size)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(Q.parameters(),lr=learning_rate)\n",
    "    buffer = ReplayBuffer(buff_size)\n",
    "    epsilon = start_epsilon\n",
    "\n",
    "    rewards, rewards_samples = [], []\n",
    "\n",
    "    print(\"Training\")\n",
    "    bar = tqdm.trange(episodes)\n",
    "    for episode in bar:\n",
    "        s,a,r = play_episode(env, Q, epsilon, buf=buffer)\n",
    "\n",
    "        if episode >= train_after:\n",
    "            for i in range(train_epochs):\n",
    "                update_Qnetworks(episode, buffer, Q, Qt, optimizer)\n",
    "\n",
    "        rews = []\n",
    "        for j in range(test_eps):\n",
    "            _, _, r = play_episode(test_env, Q, epsilon)\n",
    "            rews += [sum(r)]\n",
    "        rewards += [sum(rews)/test_eps]\n",
    "\n",
    "        if episode % (episodes-1) == 0:\n",
    "            torch.save(Q.state_dict(), f\"actors/dqn_actor_3{episode}.pth\")\n",
    "\n",
    "        # Progress bar\n",
    "        rewards_samples += [sum(rewards[-25:])/len(rewards[-25:])]\n",
    "        bar.set_description(\"Reward(%g), Epsilon(%g)\" % (rewards_samples[-1], epsilon))  \n",
    "\n",
    "        f = open('Rewards', 'a')\n",
    "        writer = csv.writer(f, lineterminator = '\\n')\n",
    "        writer.writerow([rewards_samples[-1]])\n",
    "        f.close()\n",
    "\n",
    "        epsilon = start_epsilon * (end_epsilon/start_epsilon)**(episode/episodes)\n",
    "\n",
    "    # Close once finished\n",
    "    bar.close()\n",
    "    env.close()\n",
    "    test_env.close()\n",
    "\n",
    "    return rewards_samples\n",
    "\n",
    "# Plot the mean learning curve from multiple training runs\n",
    "def plot_arrays(vars, color, label):\n",
    "    mean = np.mean(vars, axis=0)\n",
    "    std = np.std(vars, axis=0)\n",
    "    plt.plot(range(len(mean)), mean, color=color, label=label)\n",
    "    plt.fill_between(range(len(mean)), np.maximum(mean-std, -1000), np.minimum(mean+std,1000), color=color, alpha=0.3)\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    open(\"rewards\", \"w\").close()\n",
    "\n",
    "    training_runs = []\n",
    "    for i in range(1):\n",
    "        training_runs.append(train())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_arrays(training_runs, 'b', 'DQN on MountainCar')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.title(\"DQN Learning Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd27fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
