{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704bd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "class CSTRCoolingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CSTRCoolingEnv, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.V = 100             # Reactor volume (L)\n",
    "        self.q = 20             # Flowrate (L/min)\n",
    "        self.Caf = 1.0           # Feed concentration (mol/L)\n",
    "        self.Tf = 350            # Feed temperature (K)\n",
    "        self.k0 = 15.2e10         # 1/min\n",
    "        self.EoverR = 10000       # K\n",
    "        self.mdelH = 5e4         # J/mol\n",
    "        self.rho = 1000          # g/L\n",
    "        self.Cp = 0.239          # J/g-K\n",
    "        self.UA = 5e4            # J/min-K\n",
    "        self.Tj_base = 300       # K\n",
    "        self.Tj_delta = 30        # delta per action level\n",
    "        self.dt = 1.0            # time per step (min)\n",
    "        self.max_steps = 200\n",
    "        self.possible_setpoints = [290,300,310,320,330,340,350,360,370]\n",
    "\n",
    "        # Normalization ranges\n",
    "        self.Ca_min = 0.0\n",
    "        self.Ca_max = 1.2      # expected max around feed conc.\n",
    "        \n",
    "        self.T_min = 300.0\n",
    "        self.T_max = 400.0      # may vary depending on dynamics\n",
    "\n",
    "\n",
    "        # Action space: 3 discrete cooling settings\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        # Observation space: [Ca, T]\n",
    "        low_obs = np.array([0.0, 300.0, 300.0], dtype=np.float32)\n",
    "        high_obs = np.array([2.0, 500.0, 500.0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low_obs, high=high_obs, dtype=np.float32)\n",
    "\n",
    "        self.state = None\n",
    "        self.steps = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        Ca, T, sp = self.state\n",
    "\n",
    "        sp = self.setpoints[self.steps]\n",
    "\n",
    "        # Determine Tj based on action\n",
    "        if action == 0:\n",
    "            Tj = self.Tj_base + 2*self.Tj_delta\n",
    "        elif action == 1:\n",
    "            Tj = self.Tj_base + self.Tj_delta\n",
    "        elif action == 2:\n",
    "            Tj = self.Tj_base\n",
    "        elif action == 3:\n",
    "            Tj = self.Tj_base - self.Tj_delta\n",
    "        elif action == 4:\n",
    "            Tj = self.Tj_base - 2*self.Tj_delta\n",
    "\n",
    "        # Integrate ODEs for 1 time step\n",
    "        def cstr_odes(t, y):\n",
    "            Ca, T = y\n",
    "            rA = self.k0 * np.exp(-self.EoverR / T) * Ca\n",
    "            dCadt = self.q / self.V * (self.Caf - Ca) - rA\n",
    "            dTdt = (self.q / self.V * (self.Tf - T)\n",
    "                    + (-self.mdelH) / (self.rho * self.Cp) * rA\n",
    "                    + self.UA / (self.rho * self.Cp * self.V) * (Tj - T))\n",
    "            return [dCadt, dTdt]\n",
    "\n",
    "        sol = solve_ivp(cstr_odes, [0, self.dt], [Ca, T], method='RK45')\n",
    "        Ca_next, T_next = sol.y[:, -1]\n",
    "\n",
    "        self.state = np.array([Ca_next, T_next, sp], dtype=np.float32)\n",
    "\n",
    "        # Reward: penalize deviation from setpoint\n",
    "        reward = (-abs(T_next - sp))/100\n",
    "\n",
    "        self.steps += 1\n",
    "        done = self.steps >= self.max_steps\n",
    "\n",
    "        obs = self.normalize_state(*self.state)\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def generate_sectioned_list(self, C, num_sections=10, section_size=20):\n",
    "        result = []\n",
    "        for _ in range(num_sections):\n",
    "            value = random.choice(C)\n",
    "            section = [value] * section_size\n",
    "            result.extend(section)\n",
    "        return result\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        Ca0 = 1.0 \n",
    "        T0 = 350.0 + self.np_random.uniform(-5.0, 5.0)  #5\n",
    "        sp0 = 300.0\n",
    "        C = self.possible_setpoints\n",
    "        self.setpoints = self.generate_sectioned_list(C)\n",
    "        self.state = np.array([Ca0, T0, sp0], dtype=np.float32)\n",
    "        obs = self.normalize_state(*self.state)\n",
    "        return obs, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        Ca, T = self.state\n",
    "        print(f\"Step: {self.steps}, Ca: {Ca:.3f}, T: {T:.2f}\")\n",
    "\n",
    "    def normalize_state(self, Ca, T, sp):\n",
    "        Ca_norm = (Ca - self.Ca_min) / (self.Ca_max - self.Ca_min)\n",
    "        T_norm = (T - self.T_min) / (self.T_max - self.T_min)\n",
    "        sp_norm = (sp - self.T_min) / (self.T_max - self.T_min)\n",
    "        return np.array([Ca_norm, T_norm, sp_norm], dtype=np.float32)\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c67acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import warnings\n",
    "import random\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Author: Daniel Rangel-Martinez\n",
    "#Based on the work from: Pascal Poupart \n",
    "# Course: CS885 - Reinforcement Learning\n",
    "\n",
    "# Hyperparameters\n",
    "MINIBATCH_SIZE = 64\n",
    "LEARNING_RATE = 5e-5\n",
    "EPISODES = 2000\n",
    "GAMMA = 0.99\n",
    "TRAIN_AFTER_EPISODES = 50\n",
    "TRAIN_EPOCHS = 5\n",
    "BUFSIZE = 20_000\n",
    "TEST_EPISODES = 7\n",
    "HIDDEN = 100\n",
    "TARGET_UPDATE_FREQ = 3\n",
    "STARTING_EPSILON = 1.0\n",
    "EPSILON_END = 0.01\n",
    "STEPS_MAX = 500_000\n",
    "\n",
    "# Torch helper class\n",
    "class TorchHelper:\n",
    "    def __init__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"gpu\")\n",
    "            print(\"Working on CUDA\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Working on CUDA\")\n",
    "    \n",
    "    def f(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "    def i(self, x):\n",
    "        return torch.tensor(x, dtype=torch.int32).to(self.device)\n",
    "    \n",
    "    def l(self, x):\n",
    "        return torch.tensor(x, dtype=torch.long).to(self.device)\n",
    "\n",
    "t = TorchHelper()\n",
    "DEVICE = t.device\n",
    "\n",
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, N):\n",
    "        self.buf = collections.deque(maxlen=N)\n",
    "\n",
    "    def add(self, s, a, r, s2, d):\n",
    "        self.buf.append((s, a, r, s2, d))\n",
    "\n",
    "    def sample(self, n):\n",
    "        minibatch = random.sample(self.buf, n)\n",
    "        S, A, R, S2, D = zip(*minibatch)\n",
    "        return t.f(S), t.l(A), t.f(R), t.f(S2), t.i(D)\n",
    "\n",
    "# Epsilon-greedy policy\n",
    "def policy(obs, Q, epsilon):\n",
    "    obs = t.f(obs).view(-1, OBS_N)\n",
    "    if np.random.rand() < epsilon:\n",
    "        act = np.random.randint(ACT_N)\n",
    "        return act\n",
    "    else:\n",
    "        act = torch.argmax(Q(obs)).item()\n",
    "        return act\n",
    "\n",
    "# Target network update\n",
    "def update(target, source):\n",
    "    for tp, p in zip(target.parameters(), source.parameters()):\n",
    "        tp.data.copy_(p.data)\n",
    "\n",
    "# Training update\n",
    "def update_networks(epi, buf, Q, Qt, OPT):\n",
    "    S, A, R, S2, D = buf.sample(MINIBATCH_SIZE)\n",
    "    qvalues = Q(S).gather(1, A.view(-1, 1)).squeeze()\n",
    "    q2values = Qt(S2).max(dim=1).values\n",
    "    targets = R + GAMMA * q2values * (1 - D)\n",
    "    loss = torch.nn.MSELoss()(targets.detach(), qvalues)\n",
    "    OPT.zero_grad()\n",
    "    loss.backward()\n",
    "    OPT.step()\n",
    "\n",
    "    if epi % TARGET_UPDATE_FREQ == 0:\n",
    "        update(Qt, Q)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Play one episode\n",
    "def play_episode(env, Q, epsilon, buf=None, max_steps=100):\n",
    "    obs, _ = env.reset(seed=123)\n",
    "    done = False\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        act = policy(obs, Q, epsilon)\n",
    "        next_obs, reward, done, _ = env.step(act)\n",
    "\n",
    "        if buf is not None:\n",
    "            buf.add(obs, act, reward, next_obs, done)\n",
    "\n",
    "        states.append(obs)\n",
    "        actions.append(act)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    global OBS_N, ACT_N\n",
    "\n",
    "    env = CSTRCoolingEnv()\n",
    "    test_env = CSTRCoolingEnv()\n",
    "\n",
    "    OBS_N = env.observation_space.shape[0]\n",
    "    ACT_N = env.action_space.n\n",
    "\n",
    "    Q = torch.nn.Sequential(\n",
    "        torch.nn.Linear(OBS_N, HIDDEN),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, ACT_N)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    Qt = torch.nn.Sequential(\n",
    "        torch.nn.Linear(OBS_N, HIDDEN),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, ACT_N)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    OPT = torch.optim.Adam(Q.parameters(), lr=LEARNING_RATE)\n",
    "    buf = ReplayBuffer(BUFSIZE)\n",
    "    epsilon = STARTING_EPSILON\n",
    "\n",
    "    testRs, last25testRs = [], []\n",
    "\n",
    "    print(\"Training:\")\n",
    "    pbar = tqdm.trange(EPISODES)\n",
    "    for epi in pbar:\n",
    "        _,_,R = play_episode(env, Q, epsilon, buf=buf)\n",
    "\n",
    "        if epi >= TRAIN_AFTER_EPISODES:\n",
    "            for _ in range(TRAIN_EPOCHS):\n",
    "                update_networks(epi, buf, Q, Qt, OPT)\n",
    "\n",
    "        Rews = []\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            _, _, R = play_episode(test_env, Q, epsilon)\n",
    "            Rews += [sum(R)]\n",
    "        testRs += [sum(Rews)/TEST_EPISODES]\n",
    "\n",
    "        if epi  == EPISODES:\n",
    "            torch.save(Q.state_dict(), f\"dqn_actor_3{epi}.pth\")\n",
    "\n",
    "        last25testRs += [sum(testRs[-25:])/len(testRs[-25:])]\n",
    "        pbar.set_description(\"R(%g), EPSILON(%g)\" % (last25testRs[-1], epsilon)) \n",
    "\n",
    "        f = open('rewards', 'a')\n",
    "        writer = csv.writer(f, lineterminator = '\\n')\n",
    "        writer.writerow([last25testRs[-1]])\n",
    "        f.close()\n",
    "\n",
    "        epsilon = STARTING_EPSILON * (EPSILON_END/STARTING_EPSILON)**(epi/EPISODES) #max(EPSILON_END, epsilon - (2.0 / EPISODES))\n",
    "\n",
    "    pbar.close()\n",
    "    env.close()\n",
    "    test_env.close()\n",
    "\n",
    "    return last25testRs\n",
    "\n",
    "# Plot results\n",
    "def plot_arrays(vars, color, label):\n",
    "    mean = np.mean(vars, axis=0)\n",
    "    std = np.std(vars, axis=0)\n",
    "    plt.plot(range(len(mean)), mean, color=color, label=label)\n",
    "    plt.fill_between(range(len(mean)), mean - std, mean + std, color=color, alpha=0.3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    open(\"rewards\", \"w\").close()\n",
    "    \n",
    "    curves = []\n",
    "    for _ in range(1):\n",
    "        curves.append(train())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_arrays(curves, 'b', 'Rewards')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.title(\"DQN Learning Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "# Torch helper class\n",
    "class TorchHelper:\n",
    "    def __init__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Working on CPU\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Working on CUDA\")\n",
    "    \n",
    "    def f(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "    def i(self, x):\n",
    "        return torch.tensor(x, dtype=torch.int32).to(self.device)\n",
    "    \n",
    "    def l(self, x):\n",
    "        return torch.tensor(x, dtype=torch.long).to(self.device)\n",
    "\n",
    "t = TorchHelper()\n",
    "DEVICE = t.device\n",
    "\n",
    "def tester(policy):\n",
    "    Q = torch.nn.Sequential(\n",
    "        torch.nn.Linear(OBS_N, HIDDEN),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, ACT_N)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    Q.load_state_dict(policy)\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "    action = 1\n",
    "    while not done:\n",
    "        f = open('episode_test', 'a')\n",
    "        writer = csv.writer(f, lineterminator = '\\n')\n",
    "        writer.writerow([state, reward, done, action])\n",
    "        f.close()\n",
    "\n",
    "        state_ = torch.from_numpy(state)\n",
    "        action = Q(state_)  \n",
    "        action = torch.argmax(action).item()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    open('episode_test', 'w').close()             \n",
    "    HIDDEN = 100       \n",
    "    policy = torch.load(\"dqn_actor_32000.pth\")\n",
    "    env = CSTRCoolingEnv()\n",
    "    OBS_N = env.observation_space.shape[0]\n",
    "    ACT_N = env.action_space.n\n",
    "    tester(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab287b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Ca_min, Ca_max = 0.0, 1.2\n",
    "T_min, T_max = 300.0, 400.0\n",
    "\n",
    "def unnormalize_state(ca_norm, t_norm, sp_norm):\n",
    "    ca = ca_norm * (Ca_max - Ca_min) + Ca_min\n",
    "    t = t_norm * (T_max - T_min) + T_min\n",
    "    sp = sp_norm * (T_max - T_min) + T_min\n",
    "    #Tj = tj_norm * (T_max - T_min) + T_min\n",
    "    return ca, t, sp\n",
    "\n",
    "\n",
    "states_Ca = []\n",
    "states_T = []\n",
    "rewards = []\n",
    "actions = []\n",
    "setpoints = []\n",
    "\n",
    "with open(\"episode_test\", \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\"],\")\n",
    "        state_str = parts[0].replace(\"[\", \"\")\n",
    "        rest = parts[1].split(\",\", 3)\n",
    "\n",
    "        reward = float(rest[0])\n",
    "        action = int(rest[2])\n",
    "\n",
    "\n",
    "        ca_norm, t_norm, sp = map(float, state_str.strip().split())\n",
    "        ca, t, sp = unnormalize_state(ca_norm, t_norm, sp)\n",
    "\n",
    "        states_Ca.append(ca)\n",
    "        states_T.append(t)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        setpoints.append(sp)\n",
    "\n",
    "#plt.figure(figsize=(15, 12))\n",
    "\n",
    "# 1. Ca\n",
    "plt.plot(states_Ca, label=\"Ca (mol/L)\")\n",
    "plt.title(\"Concentration\")\n",
    "plt.ylabel(\"Ca (mol/L)\")\n",
    "plt.xlabel(\"Time Step (min)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 3. Reward\n",
    "plt.plot(rewards, label=\"Reward\", color='green')\n",
    "plt.title(\"Rewards obtained at each step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.xlabel(\"Time Step (min)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 4. Tj (Jacket Temp)\n",
    "\n",
    "# 5. Tj (Jacket Temp)\n",
    "plt.plot(states_T, label=\"Temperature (K)\", color='orange')\n",
    "plt.plot(setpoints, label=\"Setpoint\", color='green')\n",
    "plt.title(\"Temperature of the Reactor vs Setpoint\")\n",
    "plt.ylabel(\"Temperature (K)\")\n",
    "plt.xlabel(\"Time Step (min)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
